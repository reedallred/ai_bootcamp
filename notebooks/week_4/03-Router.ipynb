{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f138fe4c",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61cebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Prefetch, Filter, FieldCondition, MatchText, FusionQuery, Document\n",
    "\n",
    "\n",
    "from langsmith import traceable, get_current_run_tree\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.types import Send, Command\n",
    "\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "from jinja2 import Template\n",
    "from typing import Literal, Dict, Any, Annotated, List, Optional, Sequence\n",
    "from IPython.display import Image, display\n",
    "from operator import add\n",
    "from openai import OpenAI\n",
    "\n",
    "import openai\n",
    "\n",
    "import random\n",
    "import ast\n",
    "import inspect\n",
    "import instructor\n",
    "import json\n",
    "\n",
    "from utils.utils import get_tool_descriptions, format_ai_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8092fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(BaseModel):\n",
    "    expanded_query: List[str] = []\n",
    "    retrieved_context: Annotated[List[str], add] = []\n",
    "    question_relevant: bool = False\n",
    "    initial_query: str = \"\"\n",
    "    answer: str = \"\"\n",
    "    query: str = \"\"\n",
    "    k: int = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d59a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryExpandResponse(BaseModel):\n",
    "    expanded_query: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff381814",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"agent_node\",\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"gpt-4.1-mini\"}\n",
    ")\n",
    "def query_expand_node(state: State):\n",
    "\n",
    "   prompt_template =  \"\"\"You are part of a shopping assistant that can answer questions about products in stock.\n",
    "\n",
    "Instructions:\n",
    "- You will be given a question and you need to expand it into a list statements that can be used in contextual search to retrieve relevant products.\n",
    "- The statements should not overlap in context. Single item should not be expanded into multiple statements unless it is specifically mentioned in several contexts.\n",
    "\n",
    "<Question>\n",
    "{{ query }}\n",
    "</Question>\n",
    "\"\"\"\n",
    "\n",
    "   template = Template(prompt_template)\n",
    "   \n",
    "   prompt = template.render(\n",
    "      query=state.initial_query\n",
    "   )\n",
    "\n",
    "   client = instructor.from_openai(OpenAI())\n",
    "\n",
    "   response, raw_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        response_model=QueryExpandResponse,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "   )\n",
    "\n",
    "   return {\n",
    "      \"expanded_query\": response.expanded_query\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254090cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_expand_conditional_edges(state: State):\n",
    "\n",
    "    send_messages = []\n",
    "\n",
    "    for query in state.expanded_query:\n",
    "        send_messages.append(\n",
    "            Send(\n",
    "                \"retrieve_node\",\n",
    "                {\n",
    "                    \"query\": query,\n",
    "                    \"k\": 10\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return send_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9deea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"embed_query\",\n",
    "    run_type=\"embedding\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"text-embedding-3-small\"}\n",
    ")\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    response = openai.embeddings.create(\n",
    "        input=[text],\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding\n",
    "\n",
    "@traceable(\n",
    "    name=\"retrieve_top_n\",\n",
    "    run_type=\"retriever\"\n",
    ")\n",
    "def retrieve_node(state: State) -> dict:\n",
    "\n",
    "    qdrant_client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "    query_embedding = get_embedding(state[\"query\"])\n",
    "\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=\"Amazon-items-collection-01-hybrid-search\",\n",
    "        prefetch=[\n",
    "            Prefetch(\n",
    "                query=query_embedding,\n",
    "                using=\"text-embedding-3-small\",\n",
    "                limit=20\n",
    "            ),\n",
    "            Prefetch(\n",
    "                query=Document(\n",
    "                    text=state[\"query\"],\n",
    "                    model=\"qdrant/bm25\"\n",
    "                ),\n",
    "                using=\"bm25\",\n",
    "                limit=20\n",
    "            )\n",
    "        ],\n",
    "        query=FusionQuery(fusion=\"rrf\"),\n",
    "        limit=state[\"k\"],\n",
    "    )\n",
    "\n",
    "    retrieved_context = []\n",
    "    retrieved_context_ids = [] \n",
    "    retrieved_context_ratings = []\n",
    "\n",
    "    for result in results.points:\n",
    "        retrieved_context_ids.append(result.payload[\"parent_asin\"])\n",
    "        retrieved_context.append(result.payload[\"description\"])\n",
    "        retrieved_context_ratings.append(result.payload[\"average_rating\"])\n",
    "\n",
    "    formatted_context = \"\"\n",
    "\n",
    "    for id, chunk, rating in zip(retrieved_context_ids, retrieved_context, retrieved_context_ratings):\n",
    "        formatted_context += f\"- ID: {id}, rating: {rating}, description: {chunk}\\n\"\n",
    "\n",
    "    return {\n",
    "        \"retrieved_context\": [formatted_context],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e23746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregatorResponse(BaseModel):\n",
    "    answer: str = Field(description=\"Answer to the question.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4166acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"agent_node\",\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"gpt-4.1-mini\"}\n",
    ")\n",
    "def aggregator_node(state: State) -> dict:\n",
    "\n",
    "   preprocessed_context = \"\\n\".join(state.retrieved_context)\n",
    "\n",
    "   prompt_template =  \"\"\"You are a shopping assistant that can answer questions about the products in stock.\n",
    "\n",
    "You will be given a question and a list of context.\n",
    "\n",
    "Instructions:\n",
    "- You need to answer the question based on the provided context only.\n",
    "- Never use word context and refer to it as the available products.\n",
    "- The answer to the question should contain detailed information about the product and returned with detailed specification in bullet points.\n",
    "\n",
    "Context:\n",
    "{{ preprocessed_context }}\n",
    "\n",
    "Question:\n",
    "{{ question }}\n",
    "\"\"\"\n",
    "\n",
    "   template = Template(prompt_template)\n",
    "   \n",
    "   prompt = template.render(\n",
    "      preprocessed_context=preprocessed_context,\n",
    "      question=state.initial_query\n",
    "   )\n",
    "\n",
    "   client = instructor.from_openai(OpenAI())\n",
    "\n",
    "   response, raw_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        response_model=AggregatorResponse,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "   )\n",
    "\n",
    "   return {\n",
    "      \"answer\": response.answer\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf36dd2",
   "metadata": {},
   "source": [
    "### User Intent Router Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7eff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentRouterResponse(BaseModel):\n",
    "    question_relevant: bool\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d973d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"agent_node\",\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"gpt-4.1-mini\"}\n",
    ")\n",
    "def intent_router_node(state: State):\n",
    "\n",
    "   prompt_template =  \"\"\"You are part of a shopping assistant that can answer questions about products in stock.\n",
    "\n",
    "Instructions:\n",
    "- You will be given a question and you need to clasify it into relevant or not relevant.\n",
    "- If the question is not relevant, return False in field \"question_relevant\" and set \"answer\" to explanation why it is not relevant.\n",
    "- If the question is relevant, return True in field \"question_relevant\" and set \"answer\" to \"\".\n",
    "- You should only answer questions about the products in stock. If the question is not about the products in stock, you should ask for clarification.\n",
    "\n",
    "<Question>\n",
    "{{ query }}\n",
    "</Question>\n",
    "\"\"\"\n",
    "\n",
    "   template = Template(prompt_template)\n",
    "   \n",
    "   prompt = template.render(\n",
    "      query=state.initial_query\n",
    "   )\n",
    "\n",
    "   client = instructor.from_openai(OpenAI())\n",
    "\n",
    "   response, raw_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        response_model=IntentRouterResponse,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "   )\n",
    "\n",
    "   return {\n",
    "      \"question_relevant\": response.question_relevant,\n",
    "      \"answer\": response.answer\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8c3a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent_router_conditional_edges(state: State):\n",
    "\n",
    "    if state.question_relevant:\n",
    "        return \"query_expand_node\"\n",
    "    else:\n",
    "        return \"end\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7380f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"query_expand_node\", query_expand_node)\n",
    "workflow.add_node(\"retrieve_node\", retrieve_node)\n",
    "workflow.add_node(\"aggregator_node\", aggregator_node)\n",
    "workflow.add_node(\"intent_router_node\", intent_router_node)\n",
    "\n",
    "workflow.add_edge(START, \"intent_router_node\")\n",
    "workflow.add_conditional_edges(\"query_expand_node\", query_expand_conditional_edges)\n",
    "workflow.add_conditional_edges(\n",
    "    \"intent_router_node\",\n",
    "    intent_router_conditional_edges,\n",
    "    {\n",
    "        \"query_expand_node\": \"query_expand_node\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"retrieve_node\", \"aggregator_node\")\n",
    "workflow.add_edge(\"aggregator_node\", END)\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b4338",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43a39eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "    \"initial_query\": \"Can I get a tablet for my kid, a watch for me a laptop for my wife and a waterproof speaker for our party next week?\"\n",
    "}\n",
    "result = graph.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f1f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8439d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26bae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "    \"initial_query\": \"Whats the weather today?\"\n",
    "}\n",
    "result = graph.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414da6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe23dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
